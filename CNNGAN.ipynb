{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNGAN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN0pc0RHDm6TBdp2Il+JGJ3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdgroeve/ML-course-VIB-2021/blob/master/CNNGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szo-G8Pq71Y1"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import uuid\n",
        "\n",
        "\n",
        "# Configurable variables\n",
        "NUM_EPOCHS = 50\n",
        "NOISE_DIMENSION = 50\n",
        "BATCH_SIZE = 128\n",
        "TRAIN_ON_GPU = True\n",
        "UNIQUE_RUN_ID = str(uuid.uuid4())\n",
        "PRINT_STATS_AFTER_BATCH = 50\n",
        "OPTIMIZER_LR = 0.0002\n",
        "OPTIMIZER_BETAS = (0.5, 0.999)\n",
        "GENERATOR_OUTPUT_IMAGE_SHAPE = 28 * 28 * 1\n",
        "\n",
        "\n",
        "# Speed ups\n",
        "torch.autograd.set_detect_anomaly(False)\n",
        "torch.autograd.profiler.profile(False)\n",
        "torch.autograd.profiler.emit_nvtx(False)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  \"\"\"\n",
        "    Vanilla GAN Generator\n",
        "  \"\"\"\n",
        "  def __init__(self,):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      # First upsampling\n",
        "      nn.Linear(NOISE_DIMENSION, 128, bias=False),\n",
        "      nn.BatchNorm1d(128, 0.8),\n",
        "      nn.LeakyReLU(0.25),\n",
        "      # Second upsampling\n",
        "      nn.Linear(128, 256, bias=False),\n",
        "      nn.BatchNorm1d(256, 0.8),\n",
        "      nn.LeakyReLU(0.25),\n",
        "      # Third upsampling\n",
        "      nn.Linear(256, 512, bias=False),\n",
        "      nn.BatchNorm1d(512, 0.8),\n",
        "      nn.LeakyReLU(0.25),\n",
        "      # Final upsampling\n",
        "      nn.Linear(512, GENERATOR_OUTPUT_IMAGE_SHAPE, bias=False),\n",
        "      nn.Tanh()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Forward pass\"\"\"\n",
        "    return self.layers(x)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  \"\"\"\n",
        "    Vanilla GAN Discriminator\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(GENERATOR_OUTPUT_IMAGE_SHAPE, 1024), \n",
        "      nn.LeakyReLU(0.25),\n",
        "      nn.Linear(1024, 512), \n",
        "      nn.LeakyReLU(0.25),\n",
        "      nn.Linear(512, 256), \n",
        "      nn.LeakyReLU(0.25),\n",
        "      nn.Linear(256, 1),\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Forward pass\"\"\"\n",
        "    return self.layers(x)\n",
        "\n",
        "\n",
        "def get_device():\n",
        "  \"\"\" Retrieve device based on settings and availability. \"\"\"\n",
        "  return torch.device(\"cuda:0\" if torch.cuda.is_available() and TRAIN_ON_GPU else \"cpu\")\n",
        "    \n",
        "    \n",
        "def make_directory_for_run():\n",
        "  \"\"\" Make a directory for this training run. \"\"\"\n",
        "  print(f'Preparing training run {UNIQUE_RUN_ID}')\n",
        "  if not os.path.exists('./runs'):\n",
        "    os.mkdir('./runs')\n",
        "  os.mkdir(f'./runs/{UNIQUE_RUN_ID}')\n",
        "\n",
        "\n",
        "def generate_image(generator, epoch = 0, batch = 0, device=get_device()):\n",
        "  \"\"\" Generate subplots with generated examples. \"\"\"\n",
        "  images = []\n",
        "  noise = generate_noise(BATCH_SIZE, device=device)\n",
        "  generator.eval()\n",
        "  images = generator(noise)\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  for i in range(16):\n",
        "    # Get image\n",
        "    image = images[i]\n",
        "    # Convert image back onto CPU and reshape\n",
        "    image = image.cpu().detach().numpy()\n",
        "    image = np.reshape(image, (28, 28))\n",
        "    # Plot\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "  if not os.path.exists(f'./runs/{UNIQUE_RUN_ID}/images'):\n",
        "    os.mkdir(f'./runs/{UNIQUE_RUN_ID}/images')\n",
        "  plt.savefig(f'./runs/{UNIQUE_RUN_ID}/images/epoch{epoch}_batch{batch}.jpg')\n",
        "\n",
        "\n",
        "def save_models(generator, discriminator, epoch):\n",
        "  \"\"\" Save models at specific point in time. \"\"\"\n",
        "  torch.save(generator.state_dict(), f'./runs/{UNIQUE_RUN_ID}/generator_{epoch}.pth')\n",
        "  torch.save(discriminator.state_dict(), f'./runs/{UNIQUE_RUN_ID}/discriminator_{epoch}.pth')\n",
        "\n",
        "\n",
        "def print_training_progress(batch, generator_loss, discriminator_loss):\n",
        "  \"\"\" Print training progress. \"\"\"\n",
        "  print('Losses after mini-batch %5d: generator %e, discriminator %e' %\n",
        "        (batch, generator_loss, discriminator_loss))\n",
        "\n",
        "\n",
        "def prepare_dataset():\n",
        "  \"\"\" Prepare dataset through DataLoader \"\"\"\n",
        "  # Prepare MNIST dataset\n",
        "  dataset = MNIST(os.getcwd(), download=True, train=True, transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "  ]))\n",
        "  # Batch and shuffle data with DataLoader\n",
        "  trainloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "  # Return dataset through DataLoader\n",
        "  return trainloader\n",
        "\n",
        "\n",
        "def initialize_models(device = get_device()):\n",
        "  \"\"\" Initialize Generator and Discriminator models \"\"\"\n",
        "  generator = Generator()\n",
        "  discriminator = Discriminator()\n",
        "  # Move models to specific device\n",
        "  generator.to(device)\n",
        "  discriminator.to(device)\n",
        "  # Return models\n",
        "  return generator, discriminator\n",
        "\n",
        "\n",
        "def initialize_loss():\n",
        "  \"\"\" Initialize loss function. \"\"\"\n",
        "  return nn.BCELoss()\n",
        "\n",
        "\n",
        "def initialize_optimizers(generator, discriminator):\n",
        "  \"\"\" Initialize optimizers for Generator and Discriminator. \"\"\"\n",
        "  generator_optimizer = torch.optim.AdamW(generator.parameters(), lr=OPTIMIZER_LR,betas=OPTIMIZER_BETAS)\n",
        "  discriminator_optimizer = torch.optim.AdamW(discriminator.parameters(), lr=OPTIMIZER_LR,betas=OPTIMIZER_BETAS)\n",
        "  return generator_optimizer, discriminator_optimizer\n",
        "  \n",
        "\n",
        "def generate_noise(number_of_images = 1, noise_dimension = NOISE_DIMENSION, device=None):\n",
        "  \"\"\" Generate noise for number_of_images images, with a specific noise_dimension \"\"\"\n",
        "  return torch.randn(number_of_images, noise_dimension, device=device)\n",
        "\n",
        "\n",
        "def efficient_zero_grad(model):\n",
        "  \"\"\" \n",
        "    Apply zero_grad more efficiently\n",
        "    Source: https://betterprogramming.pub/how-to-make-your-pytorch-code-run-faster-93079f3c1f7b\n",
        "  \"\"\"\n",
        "  for param in model.parameters():\n",
        "    param.grad = None\n",
        "\n",
        "\n",
        "def forward_and_backward(model, data, loss_function, targets):\n",
        "  \"\"\"\n",
        "    Perform forward and backward pass in a generic way. Returns loss value.\n",
        "  \"\"\"\n",
        "  outputs = model(data)\n",
        "  error = loss_function(outputs, targets)\n",
        "  error.backward()\n",
        "  return error.item()\n",
        "\n",
        "\n",
        "def perform_train_step(generator, discriminator, real_data, \\\n",
        "  loss_function, generator_optimizer, discriminator_optimizer, device = get_device()):\n",
        "  \"\"\" Perform a single training step. \"\"\"\n",
        "  \n",
        "  # 1. PREPARATION\n",
        "  # Set real and fake labels.\n",
        "  real_label, fake_label = 1.0, 0.0\n",
        "  # Get images on CPU or GPU as configured and available\n",
        "  # Also set 'actual batch size', whih can be smaller than BATCH_SIZE\n",
        "  # in some cases.\n",
        "  real_images = real_data[0].to(device)\n",
        "  actual_batch_size = real_images.size(0)\n",
        "  label = torch.full((actual_batch_size,1), real_label, device=device)\n",
        "  \n",
        "  # 2. TRAINING THE DISCRIMINATOR\n",
        "  # Zero the gradients for discriminator\n",
        "  efficient_zero_grad(discriminator)\n",
        "  # Forward + backward on real images, reshaped\n",
        "  real_images = real_images.view(real_images.size(0), -1)\n",
        "  error_real_images = forward_and_backward(discriminator, real_images, \\\n",
        "    loss_function, label)\n",
        "  # Forward + backward on generated images\n",
        "  noise = generate_noise(actual_batch_size, device=device)\n",
        "  generated_images = generator(noise)\n",
        "  label.fill_(fake_label)\n",
        "  error_generated_images =forward_and_backward(discriminator, \\\n",
        "    generated_images.detach(), loss_function, label)\n",
        "  # Optim for discriminator\n",
        "  discriminator_optimizer.step()\n",
        "  \n",
        "  # 3. TRAINING THE GENERATOR\n",
        "  # Forward + backward + optim for generator, including zero grad\n",
        "  efficient_zero_grad(generator)\n",
        "  label.fill_(real_label)\n",
        "  error_generator = forward_and_backward(discriminator, generated_images, loss_function, label)\n",
        "  generator_optimizer.step()\n",
        "  \n",
        "  # 4. COMPUTING RESULTS\n",
        "  # Compute loss values in floats for discriminator, which is joint loss.\n",
        "  error_discriminator = error_real_images + error_generated_images\n",
        "  # Return generator and discriminator loss so that it can be printed.\n",
        "  return error_generator, error_discriminator\n",
        "  \n",
        "\n",
        "def perform_epoch(dataloader, generator, discriminator, loss_function, \\\n",
        "    generator_optimizer, discriminator_optimizer, epoch):\n",
        "  \"\"\" Perform a single epoch. \"\"\"\n",
        "  for batch_no, real_data in enumerate(dataloader, 0):\n",
        "    # Perform training step\n",
        "    generator_loss_val, discriminator_loss_val = perform_train_step(generator, \\\n",
        "      discriminator, real_data, loss_function, \\\n",
        "      generator_optimizer, discriminator_optimizer)\n",
        "    # Print statistics and generate image after every n-th batch\n",
        "    if batch_no % PRINT_STATS_AFTER_BATCH == 0:\n",
        "      print_training_progress(batch_no, generator_loss_val, discriminator_loss_val)\n",
        "      generate_image(generator, epoch, batch_no)\n",
        "  # Save models on epoch completion.\n",
        "  save_models(generator, discriminator, epoch)\n",
        "  # Clear memory after every epoch\n",
        "  torch.cuda.empty_cache()\n",
        "  \n",
        "\n",
        "def train_dcgan():\n",
        "  \"\"\" Train the DCGAN. \"\"\"\n",
        "  # Make directory for unique run\n",
        "  make_directory_for_run()\n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "  # Get prepared dataset\n",
        "  dataloader = prepare_dataset()\n",
        "  # Initialize models\n",
        "  generator, discriminator = initialize_models()\n",
        "  # Initialize loss and optimizers\n",
        "  loss_function = initialize_loss()\n",
        "  generator_optimizer, discriminator_optimizer = initialize_optimizers(generator, discriminator)\n",
        "  # Train the model\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    print(f'Starting epoch {epoch}...')\n",
        "    perform_epoch(dataloader, generator, discriminator, loss_function, \\\n",
        "      generator_optimizer, discriminator_optimizer, epoch)\n",
        "  # Finished :-)\n",
        "  print(f'Finished unique run {UNIQUE_RUN_ID}')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  train_dcgan()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdiXPRSg72eX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}